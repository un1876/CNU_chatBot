import json, re, urllib.parse ,requests,os,torch
from datetime import datetime
from dotenv import load_dotenv
from transformers import AutoTokenizer,AutoModelForSequenceClassification, GPT2LMHeadModel
from bs4 import BeautifulSoup
from textwrap import dedent
from huggingface_hub import InferenceClient

data_updated = False

load_dotenv()
token=os.getenv("TOKEN")

model_dir = "spidyun/chatbot-roberta"
chat_dir = "spidyun/kogpt2-finetuned"

#ë¶„ë¥˜ ëª¨ë¸ - RobertA
tokenizer_classification = AutoTokenizer.from_pretrained(model_dir, token=token, use_fast=True)
model_classification = AutoModelForSequenceClassification.from_pretrained(model_dir, token=token).eval()

#ì±— ë´‡ ëª¨ë¸ - KoGPT2
tokenizer = AutoTokenizer.from_pretrained(chat_dir, token=token, use_fast=True)
model = GPT2LMHeadModel.from_pretrained(chat_dir, token=token).eval()

# ë°ì´í„° ë¡œë”©
with open("../../rag_data/restaurant/menu_1.json", "r", encoding="utf-8") as f:
    fixed_menu_1 = json.load(f)
with open("../../rag_data/restaurant/menu_other.json", "r", encoding="utf-8") as f:
    daily_menu = json.load(f)
with open('../../rag_data/bus/bus.json', 'r', encoding='utf-8') as f:
    bus_stops = json.load(f)



#---------------------------ì‹ë‹¨--------------------------------------------------------------------------------------
def extract_cafeteria_from_message(message):
    """
    ë©”ì‹œì§€ì—ì„œ ì‹ë‹¹ëª… ì¶”ì¶œ: '2í•™', '3í•™', '4í•™', 'ìƒê³¼ëŒ€' ë“±ì˜ í‘œí˜„ë„ ì²˜ë¦¬
    """
    if re.search(r"(2í•™|2í•™ìƒíšŒê´€)", message):
        return "ì œ2í•™ìƒíšŒê´€"
    elif re.search(r"(3í•™|3í•™ìƒíšŒê´€)", message):
        return "ì œ3í•™ìƒíšŒê´€"
    elif re.search(r"(4í•™|4í•™ìƒíšŒê´€)", message):
        return "ì œ4í•™ìƒíšŒê´€"
    elif re.search(r"(ìƒê³¼ëŒ€|ìƒí™œê³¼í•™ëŒ€í•™)", message):
        return "ìƒí™œê³¼í•™ëŒ€í•™"
    return None

def get_meal_types_from_message_or_time(message):
    meal_keywords = {
        "ì¡°ì‹": ["ì¡°ì‹", "ì•„ì¹¨"],
        "ì¤‘ì‹": ["ì¤‘ì‹", "ì ì‹¬"],
        "ì„ì‹": ["ì„ì‹", "ì €ë…"]
    }
    detected_meals = []

    # ë©”ì‹œì§€ì— ì‹ì‚¬ëª… í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    for meal, keywords in meal_keywords.items():
        if any(word in message for word in keywords):
            detected_meals.append(meal)

    # ì•„ë¬´ê²ƒë„ ì—†ë‹¤ë©´ í˜„ì¬ ì‹œê°„ ê¸°ì¤€ 1ê°œë§Œ ë°˜í™˜
    if not detected_meals:
        now = datetime.now().time()
        if now < datetime.strptime("10:00", "%H:%M").time():
            return ["ì¡°ì‹"]
        elif now < datetime.strptime("14:00", "%H:%M").time():
            return ["ì¤‘ì‹"]
        elif now < datetime.strptime("20:00", "%H:%M").time():
            return ["ì„ì‹"]
        else:
            return []

    return detected_meals

def make_rag_context_from_fixed_menu(menu_dict):
    context_lines = []
    for category, items in menu_dict.items():
        context_lines.append(f"ğŸ“‚ {category}")
        for item, price in items.items():
            context_lines.append(f"{item}: {price}ì›")
        context_lines.append("---")
    return "\n".join(context_lines)

def rag_answer_from_fixed_menu(message):
    # (1) ë©”ë‰´ ë¡œë“œ
    menu_dict = fixed_menu_1

    # (2) Retrieval context ìƒì„±
    retrieval_context = make_rag_context_from_fixed_menu(menu_dict)

    # (3) í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = (
        "[SYSTEM] ë‹¤ìŒ 1í•™ìƒíšŒê´€ ìƒì‹œë©”ë‰´ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n"
        "[TOPIC] ì‹ë‹¨\n"
        "[RETRIEVAL]\n"
        f"{retrieval_context}\n"
        f"[USER] {message} [SEP]"
    )

    # (4) ìƒì„±
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    outputs = model.generate(
        input_ids,
        max_length=input_ids.shape[-1] + 200,
        num_beams=5,
        no_repeat_ngram_size=2,
        early_stopping=True,
        do_sample=False,
        temperature=0.5,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

    # (5) ë””ì½”ë”©
    raw = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
    return raw.strip()

def make_rag_context_from_menu(message):
    cafeteria = extract_cafeteria_from_message(message)
    if cafeteria is None:
        return "ì‹ë‹¹ ìœ„ì¹˜ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    ëŒ€ìƒ = "í•™ìƒ"
    if ("ì§ì›" in message or "êµìˆ˜" in message):
        ëŒ€ìƒ = "ì§ì›"

    ì‹ì‚¬ëª…_ë¦¬ìŠ¤íŠ¸ = get_meal_types_from_message_or_time(message)

    context_lines = [f"ì§ˆë¬¸: {message}", f"ëŒ€ìƒ: {ëŒ€ìƒ}", f"ì‹ë‹¹: {cafeteria}"]
    for meal_type in ì‹ì‚¬ëª…_ë¦¬ìŠ¤íŠ¸:
        try:
            meals = daily_menu[ëŒ€ìƒ][meal_type]
            if meals:
                today_meal = meals[0]
                if cafeteria in today_meal:
                    menu_info = today_meal[cafeteria]
                    ë©”ë‰´ = menu_info.get("ë©”ë‰´", "").strip()
                    ê°€ê²© = menu_info.get("ê°€ê²©", "").strip()
                    context_lines.append(f"{meal_type} - {cafeteria}: ë©”ë‰´: {ë©”ë‰´} / ê°€ê²©: {ê°€ê²©}")
                else:
                    context_lines.append(f"{meal_type} - {cafeteria}: ì •ë³´ ì—†ìŒ")
            else:
                context_lines.append(f"{meal_type} - ì •ë³´ ì—†ìŒ")
        except:
            context_lines.append(f"{meal_type} - ì˜¤ë¥˜ ë°œìƒ")

    return "\n".join(context_lines)

def rag_answer_from_menu(message):
    retrieval_context = make_rag_context_from_menu(message)

    prompt = (
        "[SYSTEM] '[RETRIEVAL]'í† í° ë’¤ì˜ ì •ë³´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ê³  ì•„ë˜ ì‹ë‹¨ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n"
        "[TOPIC] ì‹ë‹¨\n"
        f"[RETRIEVAL]\n{retrieval_context}\n"
        f"[USER] {message} [SEP] [BOT]"
    )

    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    outputs = model.generate(
        input_ids,
        max_length=input_ids.shape[-1] + 200,
        num_beams=5,
        no_repeat_ngram_size=2,
        early_stopping=True,
        temperature=0.5
    )

    return tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True).strip()
#--------------------------------------------ì‹ë‹¨ ë-------------------------------------------------------------



#--------------------------------------------í†µí•™/ë²„ìŠ¤-------------------------------------------------------------
def extract_bus_info_for_rag(message, service_key):
    import re
    bus_match = re.search(r'(\d{2,4})\s*ë²ˆ?', message)
    target_bus = bus_match.group(1) if bus_match else None

    if not target_bus:
        return None, None, None, "â— ë²„ìŠ¤ ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 1. ì •ë¥˜ì¥ ID ê²€ìƒ‰
    find_stops_url = "http://apis.data.go.kr/1613000/BusSttnInfoInqireService/getSttnNoList"
    params = {
        "serviceKey": service_key,
        "cityCode": 25,
        "nodeNm": message,  # ì‚¬ìš©ìê°€ ë§í•œ ë¬¸ì¥ ì „ì²´ì—ì„œ ì •ë¥˜ì¥ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡
        "numOfRows": 10,
        "pageNo": 1,
        "_type": "xml"
    }

    stop_url = find_stops_url + "?" + urllib.parse.urlencode(params, encoding="utf-8")
    stop_response = requests.get(stop_url)
    stop_soup = BeautifulSoup(stop_response.text, "xml")
    stop_items = stop_soup.find_all("item")

    if not stop_items:
        return None, None, None, "â— ì •ë¥˜ì¥ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    arrival_info_list = []
    matched_stop = None

    # 2. ê° ì •ë¥˜ì†Œ IDì— ëŒ€í•´ ë„ì°© ì •ë³´ ìš”ì²­
    for stop in stop_items:
        stop_name = stop.find("nodenm").text
        stop_id = stop.find("nodeid").text

        arrival_url = "http://apis.data.go.kr/1613000/ArvlInfoInqireService/getSttnAcctoArvlPrearngeInfoList"
        params = {
            "serviceKey": service_key,
            "cityCode": 25,
            "nodeId": stop_id,
            "numOfRows": 10,
            "pageNo": 1,
            "_type": "xml"
        }

        arrival_response = requests.get(arrival_url + "?" + urllib.parse.urlencode(params, encoding="utf-8"))
        arrival_soup = BeautifulSoup(arrival_response.text, "xml")
        items = arrival_soup.find_all("item")

        for item in items:
            routeno = item.find("routeno").text
            if routeno == target_bus:
                arrtime = int(item.find("arrtime").text)
                remain_stops = item.find("arrprevstationcnt").text
                arrival_info_list.append(
                    f"- â± {arrtime // 60}ë¶„ í›„ ë„ì°© ì˜ˆì • ({remain_stops}ê°œ ì „ ì •ë¥˜ì¥) [ì •ë¥˜ì¥: {stop_name}]"
                )
                matched_stop = stop_name

    if not arrival_info_list:
        return target_bus, None, "â— í•´ë‹¹ ë²„ìŠ¤ëŠ” í˜„ì¬ ë„ì°© ì˜ˆì •ì´ ì—†ìŠµë‹ˆë‹¤.", None

    return target_bus, matched_stop, "\n".join(arrival_info_list), None

def rag_answer_from_bus(message, tokenizer, model, service_key):
    bus_number, stop_name, arrival_info, error = extract_bus_info_for_rag(message, service_key)

    if error:
        return error

    prompt = (
        "[SYSTEM] '[RETRIEVAL]'í† í° ë’¤ì˜ ì •ë³´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ê³  ì•„ë˜ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n"
        "[TOPIC] ë²„ìŠ¤/í†µí•™\n"
        "[RETRIEVAL]\n"
        f"ë²„ìŠ¤ë²ˆí˜¸: {bus_number}\n"
        f"ì •ë¥˜ì¥ëª…: {stop_name}\n"
        f"ë„ì°© ì •ë³´:\n{arrival_info}\n"
        f"[USER] {message} [SEP] [BOT]"
    )

    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    outputs = model.generate(
        input_ids,
        max_length=input_ids.shape[-1] + 200,
        num_beams=5,
        no_repeat_ngram_size=2,
        early_stopping=True,
        temperature=0.7
    )

    return tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True).strip()

#-------------------------------------------------ë²„ìŠ¤ ìˆ˜ì •í•„ìš”-------------------------------------------------



#-------------------------------------------------ì¡¸ì—… ìš”ê±´----------------------------------------------------
def get_from_graduate(message,topic):
    old_prompt=f"[TOPIC] {topic} [USER] {message} [SEP] [BOT]"
    with open("../../rag_data/graduation_requirements/graduation_RAG.json", encoding="utf-8") as f:
        rag = json.load(f)

    # (2) dept_alias ìë™ ìƒì„± (JSON key ê¸°ë°˜)
    departments = list(rag.get(topic, {}).keys())
    suffixes = ["í•™ê³¼", "í•™ë¶€", "ëŒ€í•™", "êµìœ¡ê³¼"]

    dept_alias = {}
    for name in departments:
        aliases = {name}
        for suf in suffixes:
            if name.endswith(suf) and len(name) > len(suf):
                base = name[:-len(suf)]
                aliases.update({base, base + "í•™", base + "í•™ê³¼"})
        dept_alias[name] = list(aliases)

    # (3) old_promptì—ì„œ dept_key ì°¾ê¸°
    dept_key = next(
        (k for k, als in dept_alias.items() if any(a in old_prompt for a in als)),
        departments[0] if departments else None
    )

    # (4) í•´ë‹¹ í•™ê³¼ JSON ì¶”ì¶œ
    topic_data = rag.get(topic, {})
    dept_data = topic_data.get(dept_key, {})

    # (5) flatten into (path, text)
    chunks = []

    def walk(obj, path=[]):
        if isinstance(obj, dict):
            for k, v in obj.items(): walk(v, path + [k])
        elif isinstance(obj, list):
            for i, item in enumerate(obj): walk(item, path + [str(i)])
        else:
            chunks.append((path, str(obj)))

    walk(dept_data)

    # (6) old_promptì—ì„œ ì–´ì ˆ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = re.findall(r"[ê°€-í£0-9]+", old_prompt)

    # (7) ë¶€ë¶„ë¬¸ìì—´ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ ì²­í¬ë§Œ í•„í„°
    selected = []
    for path, text in chunks:
        if any(kw in key for kw in keywords for key in path) or any(kw in text for kw in keywords):
            selected.append((path, text))

    # (8) Retrieval ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    retrieval = "\n".join(f"{'/'.join(p)}: {t}" for p, t in selected)
    print("retrieval",retrieval)
    # (9) **SYSTEM** ì§€ì‹œë¬¸ + RAG í”„ë¡¬í”„íŠ¸
    final_prompt = (
        f"[SYSTEM] '[RETRIEVAL]'í† í° ë’¤ì˜ ì •ë³´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ê³  ì•„ë˜ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.[TOPIC] {topic} [RETRIEVAL] {retrieval}[USER] {old_prompt} [SEP]"
    )



    # (11) ìƒì„±: beam search + ë°˜ë³µ ë°©ì§€ + ë‚®ì€ temperature
    input_ids = tokenizer.encode(final_prompt, return_tensors="pt")
    outs = model.generate(
        input_ids,
        max_length=input_ids.shape[-1] + 200,
        num_beams=5,
        no_repeat_ngram_size=2,
        early_stopping=True,
        do_sample=False,
        temperature=0.5,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

    # (12) ë””ì½”ë”© & ë©”íƒ€í† í° ì œê±°
    raw = tokenizer.decode(outs[0][input_ids.shape[-1]:], skip_special_tokens=True)
    answer = raw.strip()
    return answer
#---------------------------------------------ì¡¸ì—…ìš”ê±´ë-------------------------------------------------





#---------------------------------------------í•™ì‚¬ì¼ì •---------------------------------------------------
# ì¼ë‹¨ í•™ì‚¬ì¼ì • ê°±ì‹ 

def make_rag_context_from_academic_calendar(message):
    with open("../../rag_data/canlendar/academic_calendar.json", encoding="utf-8") as f:
        calendar_data = json.load(f)

    context_lines = []
    for month_info in calendar_data:
        month = month_info["month"]
        for event in month_info["schedules"]:
            text = event["ë‚´ìš©"]
            if any(keyword in message for keyword in [month] + re.findall(r"\d{1,2}ì›”", message)):
                context_lines.append(f"{month}: {text}")
            elif re.search(r"\d{1,2}\.\d{1,2}", message) and re.search(r"\d{1,2}\.\d{1,2}", text):
                context_lines.append(f"{month}: {text}")

    if not context_lines:
        context_lines.append("ê´€ë ¨ëœ í•™ì‚¬ì¼ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return "\n".join(context_lines)

def rag_answer_from_academic_calendar(message):
    context = make_rag_context_from_academic_calendar(message)

    prompt = (
        "[SYSTEM] '[RETRIEVAL]'í† í° ë’¤ì˜ ì •ë³´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ê³  ì•„ë˜ í•™ì‚¬ì¼ì • ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n"
        "[TOPIC] í•™ì‚¬ì¼ì •\n"
        f"[RETRIEVAL]\n{context}\n"
        f"[USER] {message} [SEP] [BOT]"
    )

    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    outputs = model.generate(
        input_ids,
        max_length=input_ids.shape[-1] + 200,
        num_beams=5,
        no_repeat_ngram_size=2,
        early_stopping=True,
        temperature=0.5
    )

    return tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True).strip()
#----------------------------------------------í•™ì‚¬ì¼ì •ë--------------------------------------------------
#----------------------------------------------ê³µì§€ì‚¬í•­----------------------------------------------------


import json
from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel

# ì „ì—­ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°

def make_rag_context_from_notices(user_message, top_k=3):
    with open("../../rag_data/notice/notices.json", encoding="utf-8") as f:
        notice_data = json.load(f)["data"]

    user_message_lower = user_message.lower()

    # ê°„ë‹¨í•œ keyword matching ê¸°ë°˜ ê²€ìƒ‰ (ì¶”í›„ BM25ë‚˜ FAISSë¡œ êµì²´ ê°€ëŠ¥)
    matches = []
    for item in notice_data:
        score = 0
        title = item["title"].lower()
        content = item["content"].lower()

        if any(word in title for word in user_message_lower.split()):
            score += 1
        if any(word in content for word in user_message_lower.split()):
            score += 2

        if score > 0:
            matches.append((score, item))

    # ìƒìœ„ Kê°œ ì„ íƒ
    matches = sorted(matches, key=lambda x: x[0], reverse=True)[:top_k]

    context_lines = []
    for _, item in matches:
        context_lines.append(f"ğŸ“Œ {item['title']} ({item['date']})")
        context_lines.append(item['content'][:300].replace('\n', ' ') + "...")
        context_lines.append("")

    if not context_lines:
        context_lines.append("ê´€ë ¨ëœ ê³µì§€ì‚¬í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    return "\n".join(context_lines)

def rag_answer_for_notices(user_message):
    retrieval_context = make_rag_context_from_notices(user_message)

    prompt = (
        "[SYSTEM] '[RETRIEVAL]'í† í° ë’¤ì˜ ì •ë³´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ê³  ì•„ë˜ ê³µì§€ì‚¬í•­ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n"
        "[TOPIC] í•™êµê³µì§€ì‚¬í•­\n"
        "[RETRIEVAL]\n"
        f"{retrieval_context}\n"
        f"[USER] {user_message} [SEP] [BOT]"
    )

    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    output = model.generate(
        input_ids,
        max_length=input_ids.shape[-1] + 200,
        num_beams=5,
        no_repeat_ngram_size=2,
        early_stopping=True,
        temperature=0.8,
    )

    return tokenizer.decode(output[0][input_ids.shape[-1]:], skip_special_tokens=True).strip()
#---------------------------------------------ê³µì§€ì‚¬í•­ ë-------------------------------------------

#---------------------------------------------ë‹µë³€í•˜ê¸°--------------------------------------------

#ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ topicì„ ìš°ì„  ì¶”ì¶œ
def extract_topic_from_message(message):

    # ìë™ìœ¼ë¡œ ì €ì¥ëœ í† í¬ë‚˜ì´ì € íƒ€ì…ì„ ë¶ˆëŸ¬ì˜´
    inputs = tokenizer_classification(message, return_tensors="pt", truncation=False, padding=True)

    # ëª¨ë¸ ì˜ˆì¸¡
    with torch.no_grad():
        outputs = model_classification(**inputs)
        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=1).item()
    id2label = {0: 'ë²„ìŠ¤/í†µí•™', 1: 'ì‹ë‹¨', 2: 'ì¡¸ì—…ìš”ê±´', 3: 'í•™êµê³µì§€ì‚¬í•­', 4: 'í•™ì‚¬ì¼ì •'}
    print(f"ì˜ˆì¸¡ ë¼ë²¨: {id2label[predicted_class]}")
    topic = id2label[predicted_class]

    #ì´í›„ ì˜ˆì¸¡í•œ í† í”½ì„ return
    if topic:
        return topic
    else:
        return None

def reform(message,query,topic):
    # 1) í† í°/í”„ë¡œë°”ì´ë” ì„¤ì •
    load_dotenv()
    HF_TOKEN = os.getenv("HF_TOKEN")  # .envì— HF_TOKEN=hf_***** ë¡œ ì €ì¥
    PROVIDER = os.getenv("HF_PROVIDER", None)  # 'fireworks-ai' | 'together' | 'hf-inference' | None
    MODEL_ID = os.getenv("MODEL_ID", "openai/gpt-oss-120b")


    client = InferenceClient(model=MODEL_ID, token=HF_TOKEN, provider=PROVIDER)
    m=(message or "").strip()
    t=(topic or "").strip()
    q=(query or "").strip()
    system_promt=dedent(f"""
        ë„ˆëŠ” í•™êµ infromation ë°ìŠ¤í¬ì— ìˆëŠ” ì•ˆë‚´ì›ì²˜ëŸ¼ ì¹œì ˆí•˜ê³  ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ëŠ” AIì•¼
        ë„ˆëŠ” ê¸€ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìœ¼ë©´ í•´ë‹¹ ê¸€ì´ ì™„ì „í•˜ì§€ ì•Šë”ë¼ë„ ê·¸ ê¸€ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ìœ ì¶”í•´ì„œ ìƒì„¸í•˜ê²Œ ì™„ì„±í•´ì•¼í•´
        ì²« ë¬¸ì¥ì—ëŠ” ""{m}(ì´)ë¼ê³  ë¬¼ìœ¼ì…¨ë‹¤ë©´ ğŸ‘Š{t}ğŸ‘Šì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹œêµ°ìš” ğŸ˜!"ë¥¼ ê¼­ ë„£ì–´ì¤˜
        {q}ì˜ ë¬¸ì¥ì´ {t}ì— í•´ë‹¹ í•˜ì§€ ì•ŠëŠ” ë‹¤ë¥¸ ì£¼ì œì˜ ì–˜ê¸° ë¶€ë¶„ì´ ìˆìœ¼ë©´ ê³¼ê°í•˜ê²Œ ê·¸ ë¶€ë¶„ì€ ì‚­ì œí•´
        {m}ì—ì„œ ì‚¬ìš©ìê°€ í‘œí˜„ì„ ì–´ë–»ê²Œ í•´ì„œ ë‹µì„ í•´ë‹¤ë„ê³  í•˜ë©´ ê·¸ì— ë§ëŠ” í‘œí˜„ìœ¼ë¡œ ìµœì¢… ì¶œë ¥í•´ì£¼ê³  ë§¨ ë§ˆì§€ë§‰ì¤„ì— ê·¸ì— ë§ëŠ” í‘œí˜„ë„ ì–¸ê¸‰í•´ì¤˜
        !ì¤‘ìš” ì „ë‹¬í•´ì•¼í•˜ëŠ” ë¶€ë¶„ì´ ëë‚˜ë©´ ë”ì´ìƒ ì¶œë ¥í•˜ì§€ ë§ê³  ë©ˆì¶°
        !ë¬¸ì˜ ê´€ë ¨ ë¬¸êµ¬ë¥¼ í•  ë•ŒëŠ” https://plus.cnu.ac.kr/html/kr/ í•´ë‹¹ ì‚¬ì´íŠ¸ ë˜ëŠ” í•´ë‹¹ í•™ê³¼ì— ë¬¸ì˜í•´ë¼ê³  ë§í•´ì¤˜
        """)

    messages = [
        {"role": "system", "content": system_promt},
        {"role": "user", "content": query},
    ]

    # 5) API í˜¸ì¶œ(ì¶œë ¥)
    resp = client.chat_completion(
        messages=messages,
        max_tokens=2048,
        temperature=0.2,
    )

    # 6) ì‘ë‹µ ë°˜í™˜
    return resp.choices[0].message.content

# respond í•¨ìˆ˜ë§Œ ì´ íŒŒì¼ì— ìµœì¢…ì ìœ¼ë¡œ ë…¸ì¶œ
def respond(message, history=None):

    print("message:", message)

    if history is None:
        history = []

    topic = extract_topic_from_message(message)

    if topic == "ì‹ë‹¨":
        if "1í•™" in message or "1í•™ìƒíšŒê´€" in message:
            response = rag_answer_from_fixed_menu(message)
        else:
            response = rag_answer_from_menu(message)            # âœ…
    elif topic == "ë²„ìŠ¤/í†µí•™":
        response = rag_answer_from_bus(message, tokenizer, model, "B%2FCPbINKFaAiuYyxiX216Mwr%2F%2Ff4O%2FTySlCctcTrjW%2BsxNef73j3ahB8ZERTr6jSbj5tBF6a0S5EQ6%2F%2FmNfYOg%3D%3D") # âœ…
    elif topic == "ì¡¸ì—…ìš”ê±´":
        response = get_from_graduate(message, topic)            # âœ…
    elif topic == "í•™ì‚¬ì¼ì •":
        response = rag_answer_from_academic_calendar(message)   # âœ…
    elif topic == "ê³µì§€ì‚¬í•­":
        response = rag_answer_for_notices(message)              # âœ…

    else:
        response = "ì§€ì›ë˜ì§€ ì•ŠëŠ” ì£¼ì œì…ë‹ˆë‹¤."

    response=reform(message,response,topic)
    history.append({"role": "user", "content": message})
    history.append({"role": "assistant", "content": response})
    return "", history



# print(respond("ê²½ì œí•™ê³¼ ì¡¸ì—…ìš”ê±´ì— ëŒ€í•´ ì•Œë ¤ì¤˜"))

