import json, re, urllib.parse ,requests,os,torch
from datetime import datetime
from dotenv import load_dotenv
from transformers import AutoTokenizer,AutoModelForSequenceClassification
from bs4 import BeautifulSoup
from textwrap import dedent
from huggingface_hub import InferenceClient

data_updated = False


load_dotenv()
model_dir = "spidyun/chatbot-roberta"
BUS_KEY = os.getenv("BUS_KEY")
HF_TOKEN = os.getenv("HF_TOKEN")  # .envì— HF_TOKEN=hf_***** ë¡œ ì €ì¥
PROVIDER = os.getenv("HF_PROVIDER", None)  # 'fireworks-ai' | 'together' | 'hf-inference' | None
MODEL_ID = os.getenv("MODEL_ID", "openai/gpt-oss-120b")


#ë¶„ë¥˜ ëª¨ë¸ - RobertA
tokenizer_classification = AutoTokenizer.from_pretrained(model_dir, token=HF_TOKEN, use_fast=True)
model_classification = AutoModelForSequenceClassification.from_pretrained(model_dir, token=HF_TOKEN).eval()


# ë°ì´í„° ë¡œë”©
with open("../../rag_data/restaurant/menu_1.json", "r", encoding="utf-8") as f:
    fixed_menu_1 = json.load(f)
with open("../../rag_data/restaurant/menu_other.json", "r", encoding="utf-8") as f:
    daily_menu = json.load(f)
with open('../../rag_data/bus/bus.json', 'r', encoding='utf-8') as f:
    bus_stops = json.load(f)



#---------------------------ì‹ë‹¨--------------------------------------------------------------------------------------
def extract_cafeteria_from_message(message):
    """
    ë©”ì‹œì§€ì—ì„œ ì‹ë‹¹ëª… ì¶”ì¶œ: '2í•™', '3í•™', '4í•™', 'ìƒê³¼ëŒ€' ë“±ì˜ í‘œí˜„ë„ ì²˜ë¦¬
    """
    if re.search(r"(2í•™|2í•™ìƒíšŒê´€)", message):
        return "ì œ2í•™ìƒíšŒê´€"
    elif re.search(r"(3í•™|3í•™ìƒíšŒê´€)", message):
        return "ì œ3í•™ìƒíšŒê´€"
    elif re.search(r"(4í•™|4í•™ìƒíšŒê´€)", message):
        return "ì œ4í•™ìƒíšŒê´€"
    elif re.search(r"(ìƒê³¼ëŒ€|ìƒí™œê³¼í•™ëŒ€í•™)", message):
        return "ìƒí™œê³¼í•™ëŒ€í•™"
    return None

def get_meal_types_from_message_or_time(message):
    meal_keywords = {
        "ì¡°ì‹": ["ì¡°ì‹", "ì•„ì¹¨"],
        "ì¤‘ì‹": ["ì¤‘ì‹", "ì ì‹¬"],
        "ì„ì‹": ["ì„ì‹", "ì €ë…"]
    }
    detected_meals = []

    # ë©”ì‹œì§€ì— ì‹ì‚¬ëª… í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    for meal, keywords in meal_keywords.items():
        if any(word in message for word in keywords):
            detected_meals.append(meal)

    # ì•„ë¬´ê²ƒë„ ì—†ë‹¤ë©´ í˜„ì¬ ì‹œê°„ ê¸°ì¤€ 1ê°œë§Œ ë°˜í™˜
    if not detected_meals:
        now = datetime.now().time()
        if now < datetime.strptime("10:00", "%H:%M").time():
            return ["ì¡°ì‹"]
        elif now < datetime.strptime("14:00", "%H:%M").time():
            return ["ì¤‘ì‹"]
        elif now < datetime.strptime("20:00", "%H:%M").time():
            return ["ì„ì‹"]
        else:
            return []

    return detected_meals

def make_rag_context_from_fixed_menu(menu_dict):
    context_lines = []
    for category, items in menu_dict.items():
        context_lines.append(f"ğŸ“‚ {category}")
        for item, price in items.items():
            context_lines.append(f"{item}: {price}ì›")
        context_lines.append("---")
    return "\n".join(context_lines)



def make_rag_context_from_menu(message):
    cafeteria = extract_cafeteria_from_message(message)
    if cafeteria is None:
        return "ì‹ë‹¹ ìœ„ì¹˜ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    ëŒ€ìƒ = "í•™ìƒ"
    if ("ì§ì›" in message or "êµìˆ˜" in message):
        ëŒ€ìƒ = "ì§ì›"

    ì‹ì‚¬ëª…_ë¦¬ìŠ¤íŠ¸ = get_meal_types_from_message_or_time(message)

    context_lines = [f"ì§ˆë¬¸: {message}", f"ëŒ€ìƒ: {ëŒ€ìƒ}", f"ì‹ë‹¹: {cafeteria}"]
    for meal_type in ì‹ì‚¬ëª…_ë¦¬ìŠ¤íŠ¸:
        try:
            meals = daily_menu[ëŒ€ìƒ][meal_type]
            if meals:
                today_meal = meals[0]
                if cafeteria in today_meal:
                    menu_info = today_meal[cafeteria]
                    ë©”ë‰´ = menu_info.get("ë©”ë‰´", "").strip()
                    ê°€ê²© = menu_info.get("ê°€ê²©", "").strip()
                    context_lines.append(f"{meal_type} - {cafeteria}: ë©”ë‰´: {ë©”ë‰´} / ê°€ê²©: {ê°€ê²©}")
                else:
                    context_lines.append(f"{meal_type} - {cafeteria}: ì •ë³´ ì—†ìŒ")
            else:
                context_lines.append(f"{meal_type} - ì •ë³´ ì—†ìŒ")
        except:
            context_lines.append(f"{meal_type} - ì˜¤ë¥˜ ë°œìƒ")

    return "\n".join(context_lines)

def rag_answer_from_menu(message):
    if "1í•™" in message or "1í•™ìƒíšŒê´€" in message:
        # (1) ë©”ë‰´ ë¡œë“œ
        menu_dict = fixed_menu_1

        # (2) Retrieval context ìƒì„±
        retrieval_context = make_rag_context_from_fixed_menu(menu_dict)

    else:
        retrieval_context = make_rag_context_from_menu(message)

    return retrieval_context
#--------------------------------------------ì‹ë‹¨ ë-------------------------------------------------------------



#--------------------------------------------í†µí•™/ë²„ìŠ¤-------------------------------------------------------------
def extract_bus_info_for_rag(message, service_key):
    import re
    bus_match = re.search(r'(\d{2,4})\s*ë²ˆ?', message)
    target_bus = bus_match.group(1) if bus_match else None

    if not target_bus:
        return None, None, None, "â— ë²„ìŠ¤ ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 1. ì •ë¥˜ì¥ ID ê²€ìƒ‰
    find_stops_url = "http://apis.data.go.kr/1613000/BusSttnInfoInqireService/getSttnNoList"
    params = {
        "serviceKey": service_key,
        "cityCode": 25,
        "nodeNm": message,  # ì‚¬ìš©ìê°€ ë§í•œ ë¬¸ì¥ ì „ì²´ì—ì„œ ì •ë¥˜ì¥ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡
        "numOfRows": 10,
        "pageNo": 1,
        "_type": "xml"
    }

    stop_url = find_stops_url + "?" + urllib.parse.urlencode(params, encoding="utf-8")
    stop_response = requests.get(stop_url)
    stop_soup = BeautifulSoup(stop_response.text, "xml")
    stop_items = stop_soup.find_all("item")

    if not stop_items:
        return None, None, None, "â— ì •ë¥˜ì¥ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    arrival_info_list = []
    matched_stop = None

    # 2. ê° ì •ë¥˜ì†Œ IDì— ëŒ€í•´ ë„ì°© ì •ë³´ ìš”ì²­
    for stop in stop_items:
        stop_name = stop.find("nodenm").text
        stop_id = stop.find("nodeid").text

        arrival_url = "http://apis.data.go.kr/1613000/ArvlInfoInqireService/getSttnAcctoArvlPrearngeInfoList"
        params = {
            "serviceKey": service_key,
            "cityCode": 25,
            "nodeId": stop_id,
            "numOfRows": 10,
            "pageNo": 1,
            "_type": "xml"
        }

        arrival_response = requests.get(arrival_url + "?" + urllib.parse.urlencode(params, encoding="utf-8"))
        arrival_soup = BeautifulSoup(arrival_response.text, "xml")
        items = arrival_soup.find_all("item")

        for item in items:
            routeno = item.find("routeno").text
            if routeno == target_bus:
                arrtime = int(item.find("arrtime").text)
                remain_stops = item.find("arrprevstationcnt").text
                arrival_info_list.append(
                    f"- â± {arrtime // 60}ë¶„ í›„ ë„ì°© ì˜ˆì • ({remain_stops}ê°œ ì „ ì •ë¥˜ì¥) [ì •ë¥˜ì¥: {stop_name}]"
                )
                matched_stop = stop_name

    if not arrival_info_list:
        return target_bus, None, "â— í•´ë‹¹ ë²„ìŠ¤ëŠ” í˜„ì¬ ë„ì°© ì˜ˆì •ì´ ì—†ìŠµë‹ˆë‹¤.", None

    return target_bus, matched_stop, "\n".join(arrival_info_list), None

def rag_answer_from_bus(message, service_key):
    bus_number, stop_name, arrival_info, error = extract_bus_info_for_rag(message, service_key)

    if error:
        return error

    prompt = (
        "[RETRIEVAL]\n"
        f"ë²„ìŠ¤ë²ˆí˜¸: {bus_number}\n"
        f"ì •ë¥˜ì¥ëª…: {stop_name}\n"
        f"ë„ì°© ì •ë³´:\n{arrival_info}\n"
    )


    return prompt

#-------------------------------------------------ë²„ìŠ¤ ìˆ˜ì •í•„ìš”-------------------------------------------------



#-------------------------------------------------ì¡¸ì—… ìš”ê±´----------------------------------------------------
def get_from_graduate(message,topic):
    old_prompt=f"[TOPIC] {topic} [USER] {message} [SEP] [BOT]"
    with open("../../rag_data/graduation_requirements/graduation_RAG.json", encoding="utf-8") as f:
        rag = json.load(f)

    # (2) dept_alias ìë™ ìƒì„± (JSON key ê¸°ë°˜)
    departments = list(rag.get(topic, {}).keys())
    suffixes = ["í•™ê³¼", "í•™ë¶€", "ëŒ€í•™", "êµìœ¡ê³¼"]

    dept_alias = {}
    for name in departments:
        aliases = {name}
        for suf in suffixes:
            if name.endswith(suf) and len(name) > len(suf):
                base = name[:-len(suf)]
                aliases.update({base, base + "í•™", base + "í•™ê³¼"})
        dept_alias[name] = list(aliases)

    # (3) old_promptì—ì„œ dept_key ì°¾ê¸°
    dept_key = next(
        (k for k, als in dept_alias.items() if any(a in old_prompt for a in als)),
        departments[0] if departments else None
    )

    # (4) í•´ë‹¹ í•™ê³¼ JSON ì¶”ì¶œ
    topic_data = rag.get(topic, {})
    dept_data = topic_data.get(dept_key, {})

    # (5) flatten into (path, text)
    chunks = []

    def walk(obj, path=[]):
        if isinstance(obj, dict):
            for k, v in obj.items(): walk(v, path + [k])
        elif isinstance(obj, list):
            for i, item in enumerate(obj): walk(item, path + [str(i)])
        else:
            chunks.append((path, str(obj)))

    walk(dept_data)

    # (6) old_promptì—ì„œ ì–´ì ˆ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = re.findall(r"[ê°€-í£0-9]+", old_prompt)

    # (7) ë¶€ë¶„ë¬¸ìì—´ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ ì²­í¬ë§Œ í•„í„°
    selected = []
    for path, text in chunks:
        if any(kw in key for kw in keywords for key in path) or any(kw in text for kw in keywords):
            selected.append((path, text))

    # (8) Retrieval ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    retrieval = "\n".join(f"{'/'.join(p)}: {t}" for p, t in selected)


    return retrieval
#---------------------------------------------ì¡¸ì—…ìš”ê±´ë-------------------------------------------------





#---------------------------------------------í•™ì‚¬ì¼ì •---------------------------------------------------
# ì¼ë‹¨ í•™ì‚¬ì¼ì • ê°±ì‹ 

def make_rag_context_from_academic_calendar(message):
    with open("../../rag_data/canlendar/academic_calendar.json", encoding="utf-8") as f:
        calendar_data = json.load(f)

    context_lines = []
    for month_info in calendar_data:
        month = month_info["month"]
        for event in month_info["schedules"]:
            text = event["ë‚´ìš©"]
            if any(keyword in message for keyword in [month] + re.findall(r"\d{1,2}ì›”", message)):
                context_lines.append(f"{month}: {text}")
            elif re.search(r"\d{1,2}\.\d{1,2}", message) and re.search(r"\d{1,2}\.\d{1,2}", text):
                context_lines.append(f"{month}: {text}")

    if not context_lines:
        context_lines.append("ê´€ë ¨ëœ í•™ì‚¬ì¼ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return "\n".join(context_lines)

def rag_answer_from_academic_calendar(message):
    retrieval = make_rag_context_from_academic_calendar(message)

    return retrieval
#----------------------------------------------í•™ì‚¬ì¼ì •ë--------------------------------------------------
#----------------------------------------------ê³µì§€ì‚¬í•­----------------------------------------------------


import json
from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel

# ì „ì—­ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°

def make_rag_context_from_notices(user_message, top_k=3):
    with open("../../rag_data/notice/notices.json", encoding="utf-8") as f:
        notice_data = json.load(f)["data"]

    user_message_lower = user_message.lower()

    # ê°„ë‹¨í•œ keyword matching ê¸°ë°˜ ê²€ìƒ‰ (ì¶”í›„ BM25ë‚˜ FAISSë¡œ êµì²´ ê°€ëŠ¥)
    matches = []
    for item in notice_data:
        score = 0
        title = item["title"].lower()
        content = item["content"].lower()

        if any(word in title for word in user_message_lower.split()):
            score += 1
        if any(word in content for word in user_message_lower.split()):
            score += 2

        if score > 0:
            matches.append((score, item))

    # ìƒìœ„ Kê°œ ì„ íƒ
    matches = sorted(matches, key=lambda x: x[0], reverse=True)[:top_k]

    context_lines = []
    for _, item in matches:
        context_lines.append(f"ğŸ“Œ {item['title']} ({item['date']})")
        context_lines.append(item['content'][:300].replace('\n', ' ') + "...")
        context_lines.append("")

    if not context_lines:
        context_lines.append("ê´€ë ¨ëœ ê³µì§€ì‚¬í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    return "\n".join(context_lines)

def rag_answer_for_notices(user_message):
    retrieval = make_rag_context_from_notices(user_message)

    return retrieval
#---------------------------------------------ê³µì§€ì‚¬í•­ ë-------------------------------------------

#---------------------------------------------ë‹µë³€í•˜ê¸°--------------------------------------------

#ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ topicì„ ìš°ì„  ì¶”ì¶œ
def extract_topic_from_message(message):

    # ìë™ìœ¼ë¡œ ì €ì¥ëœ í† í¬ë‚˜ì´ì € íƒ€ì…ì„ ë¶ˆëŸ¬ì˜´
    inputs = tokenizer_classification(message, return_tensors="pt", truncation=False, padding=True)

    # ëª¨ë¸ ì˜ˆì¸¡
    with torch.no_grad():
        outputs = model_classification(**inputs)
        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=1).item()
    id2label = {0: 'ë²„ìŠ¤/í†µí•™', 1: 'ì‹ë‹¨', 2: 'ì¡¸ì—…ìš”ê±´', 3: 'í•™êµê³µì§€ì‚¬í•­', 4: 'í•™ì‚¬ì¼ì •'}
    print(f"ì˜ˆì¸¡ ë¼ë²¨: {id2label[predicted_class]}")
    topic = id2label[predicted_class]

    #ì´í›„ ì˜ˆì¸¡í•œ í† í”½ì„ return
    if topic:
        return topic
    else:
        return None
def Chatmodel(message,rag,topic):
    # 1) í† í°/í”„ë¡œë°”ì´ë” ì„¤ì •

    client = InferenceClient(model=MODEL_ID, token=HF_TOKEN, provider=PROVIDER)
    m=(message or "").strip()
    t=(topic or "").strip()
    r=(rag or "").strip()
    system_promt=dedent(f"""
        ë„ˆëŠ” í•™êµ infromation ë°ìŠ¤í¬ì— ìˆëŠ” ì•ˆë‚´ì›ì²˜ëŸ¼ ì¹œì ˆí•˜ê³  ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ëŠ” AIì•¼
        ì²« ë¬¸ì¥ì—ëŠ” ""{m}(ì´)ë¼ê³  ë¬¼ìœ¼ì…¨ë‹¤ë©´ ğŸ‘Š{t}ğŸ‘Šì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹œêµ°ìš” ğŸ˜!"ë¥¼ ê¼­ ë„£ì–´ì¤˜
        {m}ì—ì„œ ì‚¬ìš©ìê°€ í‘œí˜„ì„ ì–´ë–»ê²Œ í•´ì„œ ë‹µì„ í•´ë‹¤ë„ê³  í•˜ë©´ ê·¸ì— ë§ëŠ” í‘œí˜„ìœ¼ë¡œ ìµœì¢… ì¶œë ¥í•´ì£¼ê³  ë§¨ ë§ˆì§€ë§‰ì¤„ì— ê·¸ì— ë§ëŠ” í‘œí˜„ë„ ì–¸ê¸‰í•´ì¤˜
        !ì¤‘ìš” {r}ì˜ ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ì°¸ê³ í•˜ê³  í•´ë‹¹ ë‚´ìš©ì´ ì™„ì „í•˜ì§€ ì•Šìœ¼ë©´ ê·¼ë³¸ì ì¸ ë‚´ìš©ì€ ë°”ë€Œì§€ ì•ŠëŠ” ì„ ì—ì„œ ì¡°ê¸ˆ ë” ìœ ì¶”í•´ ë³´ê°•í•´ì¤˜
        !ì¤‘ìš” ì „ë‹¬í•´ì•¼í•˜ëŠ” ë¶€ë¶„ì´ ëë‚˜ë©´ ë”ì´ìƒ ì¶œë ¥í•˜ì§€ ë§ê³  ë©ˆì¶°
        !ë¬¸ì˜ ê´€ë ¨ ë¬¸êµ¬ë¥¼ í•  ë•ŒëŠ” https://plus.cnu.ac.kr/html/kr/ í•´ë‹¹ ì‚¬ì´íŠ¸ ë˜ëŠ” í•´ë‹¹ í•™ê³¼ì— ë¬¸ì˜í•´ë¼ê³  ë§í•´ì¤˜
        """)

    messages = [
        {"role": "system", "content": system_promt},
        {"role": "user", "content": f"ì§ˆë¬¸{m}+ì°¸ê³ ë°ì´í„°:{r}"}
    ]

    # 5) API í˜¸ì¶œ(ì¶œë ¥)
    resp = client.chat_completion(
        messages=messages,
        max_tokens=2048,
        temperature=0.2,
    )

    # 6) ì‘ë‹µ ë°˜í™˜
    return resp.choices[0].message.content


# respond í•¨ìˆ˜ë§Œ ì´ íŒŒì¼ì— ìµœì¢…ì ìœ¼ë¡œ ë…¸ì¶œ
def respond(message, history=None):

    if history is None:
        history = []

    topic = extract_topic_from_message(message)

    if topic == "ì‹ë‹¨":
        rag = rag_answer_from_menu(message)

    elif topic == "ë²„ìŠ¤/í†µí•™":
        rag = rag_answer_from_bus(message, BUS_KEY)

    elif topic == "ì¡¸ì—…ìš”ê±´":
        rag = get_from_graduate(message, topic)

    elif topic == "í•™ì‚¬ì¼ì •":
        rag = rag_answer_from_academic_calendar(message)

    elif topic == "ê³µì§€ì‚¬í•­":
        rag = rag_answer_for_notices(message)              #

    print(rag)

    response=Chatmodel(message,rag,topic)
    history.append({"role": "user", "content": message})
    history.append({"role": "assistant", "content": response})
    return "", history