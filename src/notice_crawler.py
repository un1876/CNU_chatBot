import requests,json,time,re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from pathlib import Path

class CNUNoticeCrawler:
    def __init__(self):
        self.base_url = "https://eng.cnu.ac.kr"
        self.notice_url = "https://eng.cnu.ac.kr/eng/information/notice.do"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })

    def get_notice_list(self, page=1):
        """ê³µì§€ì‚¬í•­ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
        limit = 10  # í•œ í˜ì´ì§€ë‹¹ ê²Œì‹œê¸€ ìˆ˜ (ê¸°ë³¸ê°’ì´ 10ì„)
        offset = (page - 1) * limit

        params = {
            'mode': 'list',
            'articleLimit': limit,
            'article.offset': offset
        }

        try:
            print(f"  í˜ì´ì§€ {page} ìš”ì²­ ì¤‘... (offset={offset})")
            response = self.session.get(self.notice_url, params=params, timeout=30)
            print(f"  ì‹¤ì œ ìš”ì²­ëœ URL: {response.url}")
            response.raise_for_status()
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.content, 'html.parser')
            print(f"  í˜ì´ì§€ {page} HTML íŒŒì‹± ì™„ë£Œ")
            return soup

        except requests.RequestException as e:
            print(f"  í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            print(f"  í˜ì´ì§€ {page} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None

    def extract_notice_links(self, soup):
        """ê³µì§€ì‚¬í•­ ëª©ë¡ì—ì„œ ê°œë³„ ê³µì§€ì‚¬í•­ ë§í¬ë¥¼ ì¶”ì¶œ"""
        notices = []

        try:
            # ë‹¤ì–‘í•œ í…Œì´ë¸” êµ¬ì¡° ì‹œë„
            table_selectors = [
                'table.boardList tbody tr',
                'table tbody tr',
                '.board-list tbody tr',
                '.list-table tbody tr',
                'tbody tr'
            ]

            rows = []
            for selector in table_selectors:
                rows = soup.select(selector)
                if rows:
                    print(f"    í…Œì´ë¸” ì°¾ìŒ: {selector} - {len(rows)}ê°œ í–‰")
                    break

            if not rows:
                print("    í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                # ë””ë²„ê¹…ì„ ìœ„í•´ í˜ì´ì§€ ë‚´ìš© ì¼ë¶€ ì¶œë ¥
                print("    í˜ì´ì§€ ë‚´ìš© ìƒ˜í”Œ:")
                text_content = soup.get_text()[:500]
                print(f"    {text_content}")
                return notices

            for idx, row in enumerate(rows):
                try:
                    # ì œëª© ë§í¬ ì°¾ê¸° - ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„
                    title_link = None

                    # ë°©ë²• 1: a íƒœê·¸ ì§ì ‘ ì°¾ê¸°
                    links = row.find_all('a')
                    for link in links:
                        href = link.get('href', '')
                        if 'view.do' in href or 'articleNo' in href:
                            title_link = link
                            break

                    # ë°©ë²• 2: tdì—ì„œ a íƒœê·¸ ì°¾ê¸°
                    if not title_link:
                        tds = row.find_all('td')
                        for td in tds:
                            link = td.find('a')
                            if link and link.get('href'):
                                title_link = link
                                break

                    if not title_link:
                        continue

                    href = title_link.get('href')
                    if not href:
                        continue

                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if href.startswith('/'):
                        full_url = self.base_url + href
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        full_url = urljoin(self.notice_url, href)

                    # ì œëª© ì¶”ì¶œ
                    title = title_link.get_text(strip=True)
                    if not title or len(title) < 2:
                        continue

                    # ë“±ë¡ì¼ ì¶”ì¶œ
                    date = ""
                    tds = row.find_all('td')
                    for td in tds:
                        text = td.get_text(strip=True)
                        # ë‚ ì§œ í˜•ì‹ íŒ¨í„´ë“¤ (YY.MM.DD, YYYY-MM-DD ë“±)
                        date_patterns = [
                            r'\d{2}\.\d{2}\.\d{2}',
                            r'\d{4}-\d{2}-\d{2}',
                            r'\d{4}\.\d{2}\.\d{2}',
                            r'\d{2}/\d{2}/\d{2}'
                        ]

                        for pattern in date_patterns:
                            match = re.search(pattern, text)
                            if match:
                                date = match.group()
                                break
                        if date:
                            break

                    notice_info = {
                        'title': title,
                        'url': full_url,
                        'date': date
                    }

                    notices.append(notice_info)
                    print(f"    ê³µì§€ì‚¬í•­ ì¶”ì¶œ: {title[:30]}...")

                except Exception as e:
                    print(f"    í–‰ {idx} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue

            print(f"    ì´ {len(notices)}ê°œ ê³µì§€ì‚¬í•­ ì¶”ì¶œ ì™„ë£Œ")

        except Exception as e:
            print(f"  ê³µì§€ì‚¬í•­ ë§í¬ ì¶”ì¶œ ì „ì²´ ì˜¤ë¥˜: {e}")

        return notices

    def clean_content(self, content):
        """ë‚´ìš©ì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°í•˜ê³  ì •ë¦¬"""
        if not content:
            return ""

        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ë“¤ ì œê±°
        unwanted_patterns = [
            r'ë©”ë‰´.*?ë°”ë¡œê°€ê¸°',
            r'ì£¼ìš”.*?ë°”ë¡œê°€ê¸°',
            r'Copyright.*?All rights reserved',
            r'ì´ì „ê¸€|ë‹¤ìŒê¸€',
            r'ëª©ë¡ìœ¼ë¡œ|ëª©ë¡ë³´ê¸°',
            r'ì²¨ë¶€íŒŒì¼.*?ë‹¤ìš´ë¡œë“œ',
            r'ì¡°íšŒìˆ˜.*?\d+',
            r'ë“±ë¡ì¼.*?\d{2}\.\d{2}\.\d{2}',
            r'ì‘ì„±ì.*?ê³µê³¼ëŒ€í•™',
            r'HOME.*?ê³µì§€ì‚¬í•­',
            r'QUICK MENU.*',
            r'SITEMAP.*',
            r'ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨.*',
            r'í™ˆí˜ì´ì§€.*?ìš´ì˜ì •ì±…',
            r'SNS.*?ë°”ë¡œê°€ê¸°',
            r'ì „ì²´ë©”ë‰´.*',
            r'ê²€ìƒ‰.*?ë²„íŠ¼',
            r'Language.*?Translation',
            r'ë²ˆí˜¸\s*ì œëª©\s*ì²¨ë¶€\s*ì‘ì„±ì\s*ë“±ë¡ì¼\s*ì¡°íšŒìˆ˜',
            r'í˜ì´ì§€.*?ì´ë™',
            r'ì´ì „\s*ë‹¤ìŒ\s*ëª©ë¡'
        ]

        # íŒ¨í„´ ì œê±°
        cleaned = content
        for pattern in unwanted_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)

        # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
        cleaned = re.sub(r'\n\s*\n', '\n', cleaned)  # ë¹ˆ ì¤„ ì •ë¦¬
        cleaned = re.sub(r'\s+', ' ', cleaned)  # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        cleaned = re.sub(r'\n+', '\n', cleaned)  # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬

        # ì•ë’¤ ê³µë°± ì œê±°
        cleaned = cleaned.strip()

        # ë„ˆë¬´ ì§§ì€ ë‚´ìš©ì€ ì œì™¸
        if len(cleaned) < 20:
            return ""

        return cleaned

    def is_title_duplicate(self, content, title):
        """ë‚´ìš©ì´ ì œëª©ê³¼ ì¤‘ë³µë˜ëŠ”ì§€ í™•ì¸"""
        if not content or not title:
            return False

        # ì œëª©ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ê³  ë¹„êµ
        title_clean = re.sub(r'[^\w\s]', '', title)
        content_clean = re.sub(r'[^\w\s]', '', content)

        # ë‚´ìš©ì˜ ì²« ë¶€ë¶„ì´ ì œëª©ê³¼ 70% ì´ìƒ ì¼ì¹˜í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
        content_first = content_clean[:len(title_clean)]

        # ë¬¸ì ë‹¨ìœ„ë¡œ ì¼ì¹˜ë„ ê³„ì‚°
        matches = sum(1 for a, b in zip(title_clean.lower(), content_first.lower()) if a == b)
        similarity = matches / len(title_clean) if len(title_clean) > 0 else 0

        return similarity > 0.7

    def get_notice_content(self, notice_url, notice_title=""):
        """ê°œë³„ ê³µì§€ì‚¬í•­ì˜ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ê°œì„ ë¨)"""
        try:
            print(f"      ë‚´ìš© ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
            response = self.session.get(notice_url, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.content, 'html.parser')

            # ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ì„ ë¨¼ì € ì œê±°
            unwanted_selectors = [
                'nav', 'header', 'footer', '.nav', '.header', '.footer',
                '.menu', '.gnb', '.lnb', '.snb', '.breadcrumb',
                '.btn', '.button', 'button',
                '.quick-menu', '.sitemap', '.copyright',
                '.social', '.sns', '.share',
                'script', 'style', 'noscript',
                '.prev-next', '.board-nav'  # ì´ì „/ë‹¤ìŒ ë„¤ë¹„ê²Œì´ì…˜ ì œê±°
            ]

            for selector in unwanted_selectors:
                for elem in soup.select(selector):
                    elem.decompose()

            # ë””ë²„ê¹…: í˜ì´ì§€ êµ¬ì¡° ë¶„ì„
            print(f"      í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ ì¤‘...")

            # 1ë‹¨ê³„: ë” êµ¬ì²´ì ì¸ ì„ íƒìë“¤ë¡œ ë‚´ìš© ì°¾ê¸°
            content_selectors = [
                # ê²Œì‹œíŒ ìƒì„¸ë³´ê¸° ì „ìš© ì„ íƒìë“¤
                '.board-view .view-content',
                '.board-detail .content',
                '.article-view .content',
                '.post-view .content',
                '.notice-view .content',

                # í…Œì´ë¸” ê¸°ë°˜ ê²Œì‹œíŒ (ë” êµ¬ì²´ì )
                'table.view-table tr:has(td[colspan]) td',
                'table.boardView tr:has(td[colspan]) td',
                'table.detailView tr:has(td[colspan]) td',

                # ì¼ë°˜ì ì¸ ë‚´ìš© ì˜ì—­
                '.view-content',
                '.article-content',
                '.post-content',
                '.content-area',
                '.board-content',

                # ID ê¸°ë°˜ ì„ íƒì
                '#content .view',
                '#articleContent',
                '#postContent'
            ]

            content = ""
            content_found = False

            for selector in content_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    text = elem.get_text(separator='\n', strip=True)

                    # ì¶©ë¶„í•œ ë‚´ìš©ì´ ìˆê³ , ì œëª©ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸
                    if (len(text) > 50 and
                            not self.is_title_duplicate(text, notice_title) and
                            any(char in text for char in '.,!?')):
                        content = text
                        content_found = True
                        print(f"      ë‚´ìš© ë°œê²¬: {selector}")
                        break

                if content_found:
                    break

            # 2ë‹¨ê³„: í…Œì´ë¸”ì—ì„œ colspanì´ ìˆëŠ” td ì°¾ê¸° (ë” ì •í™•í•œ ë°©ë²•)
            if not content_found:
                print(f"      í…Œì´ë¸”ì—ì„œ colspan ê¸°ë°˜ ë‚´ìš© ê²€ìƒ‰...")

                # colspan ì†ì„±ì´ ìˆëŠ” tdëŠ” ë³´í†µ ë‚´ìš©ì„ ë‹´ê³  ìˆìŒ
                colspan_tds = soup.find_all('td', {'colspan': True})

                for td in colspan_tds:
                    text = td.get_text(separator='\n', strip=True)
                    if (len(text) > 100 and
                            not self.is_title_duplicate(text, notice_title) and
                            'ê³µì§€ì‚¬í•­' not in text[:50]):  # í—¤ë” ë¶€ë¶„ ì œì™¸
                        content = text
                        content_found = True
                        print(f"      colspan í…Œì´ë¸”ì—ì„œ ë‚´ìš© ë°œê²¬")
                        break

            # 3ë‹¨ê³„: ëª¨ë“  tdì—ì„œ ê°€ì¥ ê¸´ ì˜ë¯¸ìˆëŠ” ë‚´ìš© ì°¾ê¸°
            if not content_found:
                print(f"      ì „ì²´ í…Œì´ë¸”ì—ì„œ ë‚´ìš© ê²€ìƒ‰...")

                tds = soup.find_all('td')
                candidates = []

                for td in tds:
                    text = td.get_text(separator='\n', strip=True)

                    # ì‹¤ì œ ë‚´ìš©ìœ¼ë¡œ ë³´ì´ëŠ” ì¡°ê±´ë“¤
                    if (len(text) > 80 and
                            not self.is_title_duplicate(text, notice_title) and
                            not any(skip in text for skip in ['ë²ˆí˜¸', 'ì œëª©', 'ì‘ì„±ì', 'ì¡°íšŒìˆ˜', 'ë“±ë¡ì¼']) and
                            any(keyword in text for keyword in
                                ['ì¼ì‹œ', 'ì¥ì†Œ', 'ëŒ€ìƒ', 'ì•ˆë‚´', 'ì‹ ì²­', 'ë¬¸ì˜', 'ê°œìµœ', 'ì‹¤ì‹œ', 'ë°”ëë‹ˆë‹¤', 'í•˜ì˜¤ë‹ˆ', 'ì°¸ê°€', 'ì ‘ìˆ˜'])):
                        candidates.append((text, len(text)))

                # ê°€ì¥ ê¸´ í›„ë³´ë¥¼ ì„ íƒ
                if candidates:
                    content = max(candidates, key=lambda x: x[1])[0]
                    content_found = True
                    print(f"      í…Œì´ë¸”ì—ì„œ {len(candidates)}ê°œ í›„ë³´ ì¤‘ ìµœì  ë‚´ìš© ì„ íƒ")

            # 4ë‹¨ê³„: ì „ì²´ í˜ì´ì§€ì—ì„œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ë‚´ìš© ì¶”ì¶œ
            if not content_found:
                print(f"      ì „ì²´ í˜ì´ì§€ íŒ¨í„´ ë§¤ì¹­...")
                body_text = soup.get_text(separator='\n', strip=True)
                lines = body_text.split('\n')

                content_lines = []
                start_collecting = False

                for line in lines:
                    line = line.strip()

                    # ë‚´ìš© ì‹œì‘ì„ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œë“¤
                    start_keywords = ['ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤', 'ì•Œë ¤ë“œë¦½ë‹ˆë‹¤', 'ê³µì§€í•©ë‹ˆë‹¤', 'ê°œìµœí•©ë‹ˆë‹¤']

                    if any(keyword in line for keyword in start_keywords):
                        start_collecting = True

                    if start_collecting and len(line) > 10:
                        # ë¶ˆí•„ìš”í•œ ë¼ì¸ ì œì™¸
                        if not any(skip in line for skip in ['ë©”ë‰´', 'ë°”ë¡œê°€ê¸°', 'Copyright', 'ì¡°íšŒìˆ˜', 'ì´ì „ê¸€', 'ë‹¤ìŒê¸€']):
                            content_lines.append(line)

                    # ë„ˆë¬´ ë§ì´ ìˆ˜ì§‘í•˜ë©´ ì¤‘ë‹¨
                    if len(content_lines) > 20:
                        break

                if content_lines:
                    content = '\n'.join(content_lines)
                    print(f"      íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ {len(content_lines)}ì¤„ ì¶”ì¶œ")

            # 5ë‹¨ê³„: ìµœí›„ì˜ ìˆ˜ë‹¨ - í˜ì´ì§€ì—ì„œ íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ ë¬¸ë‹¨ ì¶”ì¶œ
            if not content:
                print(f"      í‚¤ì›Œë“œ ê¸°ë°˜ ìµœì¢… ì¶”ì¶œ ì‹œë„...")

                # ëª¨ë“  p íƒœê·¸ì™€ div íƒœê·¸ì—ì„œ ê²€ìƒ‰
                for tag in soup.find_all(['p', 'div']):
                    text = tag.get_text(strip=True)
                    if (len(text) > 50 and
                            any(keyword in text for keyword in ['ì¼ì‹œ:', 'ì¥ì†Œ:', 'ëŒ€ìƒ:', 'ë¬¸ì˜:', 'ì‹ ì²­']) and
                            not self.is_title_duplicate(text, notice_title)):
                        content = text
                        print(f"      í‚¤ì›Œë“œ ê¸°ë°˜ ë‚´ìš© ë°œê²¬")
                        break

            # ë‚´ìš© ì •ë¦¬
            if content:
                content = self.clean_content(content)

                # ìµœì¢… ì¤‘ë³µ ì²´í¬
                if self.is_title_duplicate(content, notice_title):
                    print(f"      âš ï¸  ì¶”ì¶œëœ ë‚´ìš©ì´ ì œëª©ê³¼ ì¤‘ë³µë¨, ë‚´ìš© ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬")
                    content = ""

                # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                if len(content) > 1500:
                    content = content[:1500] + "..."

            if not content:
                print(f"      âŒ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            else:
                print(f"      âœ… ë‚´ìš© ì¶”ì¶œ ì™„ë£Œ ({len(content)}ì)")

            return content

        except requests.RequestException as e:
            print(f"      ë‚´ìš© ìš”ì²­ ì‹¤íŒ¨: {e}")
            return ""
        except Exception as e:
            print(f"      ë‚´ìš© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return ""

    def crawl_notices(self, max_pages=10):
        """ê³µì§€ì‚¬í•­ì„ í¬ë¡¤ë§í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
        all_notices = []

        print(f"=== ì¶©ë‚¨ëŒ€í•™êµ ê³µê³¼ëŒ€í•™ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ì‹œì‘ ===")
        print(f"í¬ë¡¤ë§ ëŒ€ìƒ: ìµœëŒ€ {max_pages}í˜ì´ì§€")
        print(f"ëŒ€ìƒ URL: {self.notice_url}")

        for page in range(1, max_pages + 1):
            print(f"\n[í˜ì´ì§€ {page}/{max_pages}]")

            # ê³µì§€ì‚¬í•­ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            soup = self.get_notice_list(page)
            if not soup:
                print(f"  í˜ì´ì§€ {page} ê±´ë„ˆëœ€")
                continue

            # ê³µì§€ì‚¬í•­ ë§í¬ ì¶”ì¶œ
            notices = self.extract_notice_links(soup)
            if not notices:
                print(f"  í˜ì´ì§€ {page}ì—ì„œ ê³µì§€ì‚¬í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                continue

            print(f"  í˜ì´ì§€ {page}ì—ì„œ {len(notices)}ê°œì˜ ê³µì§€ì‚¬í•­ ë°œê²¬")

            # ê° ê³µì§€ì‚¬í•­ì˜ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            for i, notice in enumerate(notices, 1):
                print(f"    [{i}/{len(notices)}] {notice['title'][:40]}...")

                # ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ì œëª©ë„ í•¨ê»˜ ì „ë‹¬)
                content = self.get_notice_content(notice['url'], notice['title'])

                notice_data = {
                    'page': page,
                    'index': i,
                    'title': notice['title'],
                    'date': notice['date'],
                    'url': notice['url'],
                    'content': content
                }

                all_notices.append(notice_data)

                # ì„œë²„ ê³¼ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
                time.sleep(2)

        return all_notices

    def save_to_json(self, notices, filename='notices.json'):
        """í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # ìš”ì•½ ì •ë³´ ì¶”ê°€
            summary = {
                'crawl_date': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_count': len(notices),
                'source_url': self.notice_url,
                'data': notices
            }

            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)

            print(f"\n=== í¬ë¡¤ë§ ì™„ë£Œ ===")
            print(f"ì €ì¥ íŒŒì¼: {filename}")
            print(f"ì´ ê³µì§€ì‚¬í•­ ìˆ˜: {len(notices)}ê°œ")

            # ë‚´ìš©ì´ ìˆëŠ” ê³µì§€ì‚¬í•­ê³¼ ì—†ëŠ” ê³µì§€ì‚¬í•­ í†µê³„
            with_content = sum(1 for notice in notices if notice['content'])
            without_content = len(notices) - with_content
            print(f"ë‚´ìš© ì¶”ì¶œ ì„±ê³µ: {with_content}ê°œ")
            print(f"ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {without_content}ê°œ")

            # ìƒ˜í”Œ ì¶œë ¥
            if notices:
                print(f"\n=== ìƒ˜í”Œ ë°ì´í„° ===")
                sample = notices[0]
                print(f"ì œëª©: {sample['title']}")
                print(f"ë‚ ì§œ: {sample['date']}")
                print(f"URL: {sample['url']}")
                print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {sample['content'][:100]}...")

            return True

        except Exception as e:
            print(f"JSON ì €ì¥ ì˜¤ë¥˜: {e}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ì¶©ë‚¨ëŒ€í•™êµ ê³µê³¼ëŒ€í•™ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬ v2.1 (ê°œì„ ëœ ë‚´ìš© ì¶”ì¶œ)")
    print("=" * 60)

    crawler = CNUNoticeCrawler()

    try:
        # 3í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§
        notices = crawler.crawl_notices(max_pages=10)

        if notices:
            # JSON íŒŒì¼ë¡œ ì €ì¥
            SAVE_DIR = Path("../rag_data/notice")
            out_path = SAVE_DIR / "notices.json"

            success = crawler.save_to_json(notices, filename=str(out_path))

            if success:
                print(f"\nâœ… í¬ë¡¤ë§ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                print(f"ğŸ“ íŒŒì¼ ìœ„ì¹˜: ./notices.json")
            else:
                print(f"\nâŒ íŒŒì¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"\nâš ï¸  í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆê±°ë‚˜ ì ‘ê·¼ì´ ì œí•œë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    except KeyboardInterrupt:
        print(f"\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


if __name__ == "__main__":
    main()