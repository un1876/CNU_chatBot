# pip install -U "huggingface_hub>=0.33" python-dotenv

import os
from textwrap import dedent

from dotenv import load_dotenv
from huggingface_hub import InferenceClient


def reform(message,query,topic):
    # 1) í† í°/í”„ë¡œë°”ì´ë” ì„¤ì •
    load_dotenv()
    HF_TOKEN = os.getenv("HF_TOKEN")  # .envì— HF_TOKEN=hf_***** ë¡œ ì €ì¥
    PROVIDER = os.getenv("HF_PROVIDER", None)  # 'fireworks-ai' | 'together' | 'hf-inference' | None

    # 2) ì‚¬ìš©í•  ëª¨ë¸ (120Bê°€ ëŠë¦¬ë©´ 20B ê¶Œì¥: "openai/gpt-oss-20b")
    MODEL_ID = os.getenv("MODEL_ID", "openai/gpt-oss-120b")

    # 3) í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = InferenceClient(model=MODEL_ID, token=HF_TOKEN, provider=PROVIDER)
    m=(message or "").strip()
    t=(topic or "").strip()
    q=(query or "").strip()
    system_promt=dedent(f"""
        ë„ˆëŠ” í•™êµ infromation ë°ìŠ¤í¬ì— ìˆëŠ” ì•ˆë‚´ì›ì²˜ëŸ¼ ì¹œì ˆí•˜ê³  ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ëŠ” AIì•¼
        ë„ˆëŠ” ê¸€ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìœ¼ë©´ í•´ë‹¹ ê¸€ì´ ì™„ì „í•˜ì§€ ì•Šë”ë¼ë„ ê·¸ ê¸€ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ìœ ì¶”í•´ì„œ ìƒì„¸í•˜ê²Œ ì™„ì„±í•´ì•¼í•´
        ì²« ë¬¸ì¥ì—ëŠ” ""{m}(ì´)ë¼ê³  ë¬¼ìœ¼ì…¨ë‹¤ë©´ ğŸ‘Š{t}ğŸ‘Šì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹œêµ°ìš” ğŸ˜!"ë¥¼ ê¼­ ë„£ì–´ì¤˜
        ë§Œ{q}ì—ì„œ ë“¤ì–´ì˜¨ ê¸€ìê°€ {t}ì™€ ë§ì§€ ì•ŠëŠ” ë‹¤ë¥¸ ì£¼ì œì˜ ì–˜ê¸°ì¸ ë¶€ë¶„ì€ ê³¼ê°í•˜ê²Œ ì œê±°í•´
        {m}ì—ì„œ ì‚¬ìš©ìê°€ í‘œí˜„ì„ ì–´ë–»ê²Œ í•´ì„œ ë‹µì„ í•´ë‹¤ë„ê³  í•˜ë©´ ê·¸ì— ë§ëŠ” í‘œí˜„ìœ¼ë¡œ ìµœì¢… ì¶œë ¥í•´ì£¼ê³  ë§¨ ë§ˆì§€ë§‰ì¤„ì— ê·¸ì— ë§ëŠ” í‘œí˜„ë„ ì–¸ê¸‰í•´ì¤˜
        !ì¤‘ìš” ì „ë‹¬í•´ì•¼í•˜ëŠ” ë¶€ë¶„ì´ ëë‚˜ë©´ ë”ì´ìƒ ì¶œë ¥í•˜ì§€ ë§ê³  ë©ˆì¶°
        """)
    # 4) ëŒ€í™” ë©”ì‹œì§€(ì…ë ¥)
    messages = [
        {"role": "system", "content": system_promt},
        {"role": "user", "content": query},
    ]

    # 5) API í˜¸ì¶œ(ì¶œë ¥)
    resp = client.chat_completion(
        messages=messages,
        max_tokens=2048,
        temperature=0.2,
    )

    # 6) ì‘ë‹µ ë°˜í™˜
    return resp.choices[0].message.content
