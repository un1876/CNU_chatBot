# chatbot_core.py
import json, re, urllib.parse, requests
from bs4 import BeautifulSoup
from datetime import datetime
import torch
from transformers import AutoTokenizer, ElectraForSequenceClassification, PreTrainedTokenizerFast, GPT2LMHeadModel
from notice_crawler import CNUNoticeCrawler

# ì „ì—­ ëª¨ë¸ ë¡œë“œ
tokenizer = PreTrainedTokenizerFast.from_pretrained("../kogpt2-finetuned")
model = GPT2LMHeadModel.from_pretrained("../kogpt2-finetuned").eval()

# ë°ì´í„° ë¡œë”©
with open("menu_1.json", "r", encoding="utf-8") as f:
    fixed_menu_1 = json.load(f)
with open("menu_other.json", "r", encoding="utf-8") as f:
    daily_menu = json.load(f)
with open('bus.json', 'r', encoding='utf-8') as f:
    bus_stops = json.load(f)

from academic_crawler import crawl_academic_calendar




# --- í•„ìš”í•œ í•¨ìˆ˜ë“¤ ë³µì‚¬ ---
# 2) get_from_graduate
# 3) rag_answer_for_notices

# 5) answer_from_daily_menu
# 6) get_bus_arrival_by_number
# 7) chatbot_pipeline
# ...
#---------------------------ì‹ë‹¨--------------------------------------------------------------------------------------
def extract_cafeteria_from_message(message):
    """
    ë©”ì‹œì§€ì—ì„œ ì‹ë‹¹ëª… ì¶”ì¶œ: '2í•™', '3í•™', '4í•™', 'ìƒê³¼ëŒ€' ë“±ì˜ í‘œí˜„ë„ ì²˜ë¦¬
    """
    if re.search(r"(2í•™|2í•™ìƒíšŒê´€)", message):
        return "ì œ2í•™ìƒíšŒê´€"
    elif re.search(r"(3í•™|3í•™ìƒíšŒê´€)", message):
        return "ì œ3í•™ìƒíšŒê´€"
    elif re.search(r"(4í•™|4í•™ìƒíšŒê´€)", message):
        return "ì œ4í•™ìƒíšŒê´€"
    elif re.search(r"(ìƒê³¼ëŒ€|ìƒí™œê³¼í•™ëŒ€í•™)", message):
        return "ìƒí™œê³¼í•™ëŒ€í•™"
    return None
def get_meal_types_from_message_or_time(message):
    meal_keywords = {
        "ì¡°ì‹": ["ì¡°ì‹", "ì•„ì¹¨"],
        "ì¤‘ì‹": ["ì¤‘ì‹", "ì ì‹¬"],
        "ì„ì‹": ["ì„ì‹", "ì €ë…"]
    }
    detected_meals = []

    # ë©”ì‹œì§€ì— ì‹ì‚¬ëª… í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    for meal, keywords in meal_keywords.items():
        if any(word in message for word in keywords):
            detected_meals.append(meal)

    # ì•„ë¬´ê²ƒë„ ì—†ë‹¤ë©´ í˜„ì¬ ì‹œê°„ ê¸°ì¤€ 1ê°œë§Œ ë°˜í™˜
    if not detected_meals:
        now = datetime.now().time()
        if now < datetime.strptime("10:00", "%H:%M").time():
            return ["ì¡°ì‹"]
        elif now < datetime.strptime("14:00", "%H:%M").time():
            return ["ì¤‘ì‹"]
        elif now < datetime.strptime("20:00", "%H:%M").time():
            return ["ì„ì‹"]
        else:
            return []

    return detected_meals

def make_rag_context_from_fixed_menu(menu_dict):
    context_lines = []
    for category, items in menu_dict.items():
        context_lines.append(f"ğŸ“‚ {category}")
        for item, price in items.items():
            context_lines.append(f"{item}: {price}ì›")
        context_lines.append("---")
    return "\n".join(context_lines)

def rag_answer_from_fixed_menu(message):
    # (1) ë©”ë‰´ ë¡œë“œ
    menu_dict = fixed_menu_1

    # (2) Retrieval context ìƒì„±
    retrieval_context = make_rag_context_from_fixed_menu(menu_dict)

    # (3) í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = (
        "[SYSTEM] ë‹¤ìŒ 1í•™ìƒíšŒê´€ ìƒì‹œë©”ë‰´ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n"
        "[TOPIC] ì‹ë‹¨\n"
        "[RETRIEVAL]\n"
        f"{retrieval_context}\n"
        f"[USER] {message} [SEP]"
    )

    # (4) ìƒì„±
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    outputs = model.generate(
        input_ids,
        max_length=input_ids.shape[-1] + 100,
        num_beams=5,
        no_repeat_ngram_size=2,
        early_stopping=True,
        do_sample=False,
        temperature=0.5,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

    # (5) ë””ì½”ë”©
    raw = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
    return raw.strip()

def make_rag_context_from_menu(message):
    cafeteria = extract_cafeteria_from_message(message)
    if cafeteria is None:
        return "ì‹ë‹¹ ìœ„ì¹˜ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    ëŒ€ìƒ = "í•™ìƒ"
    if ("ì§ì›" in message or "êµìˆ˜" in message):
        ëŒ€ìƒ = "ì§ì›"

    ì‹ì‚¬ëª…_ë¦¬ìŠ¤íŠ¸ = get_meal_types_from_message_or_time(message)

    context_lines = [f"ì§ˆë¬¸: {message}", f"ëŒ€ìƒ: {ëŒ€ìƒ}", f"ì‹ë‹¹: {cafeteria}"]
    for meal_type in ì‹ì‚¬ëª…_ë¦¬ìŠ¤íŠ¸:
        try:
            meals = daily_menu[ëŒ€ìƒ][meal_type]
            if meals:
                today_meal = meals[0]
                if cafeteria in today_meal:
                    menu_info = today_meal[cafeteria]
                    ë©”ë‰´ = menu_info.get("ë©”ë‰´", "").strip()
                    ê°€ê²© = menu_info.get("ê°€ê²©", "").strip()
                    context_lines.append(f"{meal_type} - {cafeteria}: ë©”ë‰´: {ë©”ë‰´} / ê°€ê²©: {ê°€ê²©}")
                else:
                    context_lines.append(f"{meal_type} - {cafeteria}: ì •ë³´ ì—†ìŒ")
            else:
                context_lines.append(f"{meal_type} - ì •ë³´ ì—†ìŒ")
        except:
            context_lines.append(f"{meal_type} - ì˜¤ë¥˜ ë°œìƒ")

    return "\n".join(context_lines)

def rag_answer_from_menu(message):
    retrieval_context = make_rag_context_from_menu(message)

    prompt = (
        "[SYSTEM] ì•„ë˜ ì‹ë‹¨ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
        "[TOPIC] ì‹ë‹¨\n"
        f"[RETRIEVAL]\n{retrieval_context}\n"
        f"[USER] {message} [SEP] [BOT]"
    )

    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    outputs = model.generate(
        input_ids,
        max_length=input_ids.shape[-1] + 100,
        num_beams=5,
        no_repeat_ngram_size=2,
        early_stopping=True,
        temperature=0.5
    )

    return tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True).strip()
#--------------------------------------------ì‹ë‹¨ ë-------------------------------------------------------------



#--------------------------------------------í†µí•™/ë²„ìŠ¤-------------------------------------------------------------
def extract_bus_info_for_rag(message):
    """
    ë©”ì‹œì§€ì—ì„œ ë²„ìŠ¤ ë²ˆí˜¸ì™€ ì •ë¥˜ì¥ëª… ì¶”ì¶œ í›„, ê´€ë ¨ ë„ì°© ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜
    """
    import re
    bus_match = re.search(r'(\d{2,4})\s*ë²ˆ?', message)
    target_bus = bus_match.group(1) if bus_match else None

    stop_name = None
    for stop in bus_stops:
        if stop["ì •ë¥˜ì¥ëª…"] in message:
            stop_name = stop["ì •ë¥˜ì¥ëª…"]
            stop_id = stop["ì •ë¥˜ì¥ID"]
            break

    if not target_bus or not stop_name:
        return None, None, None, "ë²„ìŠ¤ ë²ˆí˜¸ ë˜ëŠ” ì •ë¥˜ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # API ìš”ì²­
    url = f"http://openapitraffic.daejeon.go.kr/api/rest/busposinfo/getBusPosByRtid"
    params = {
        "serviceKey": "B%2FCPbINKFaAiuYyxiX216Mwr%2F%2Ff4O%2FTySlCctcTrjW%2BsxNef73j3ahB8ZERTr6jSbj5tBF6a0S5EQ6%2F%2FmNfYOg%3D%3D",  # ì´ ë¶€ë¶„ ì±„ì›Œì¤˜ì•¼ í•¨
        "busRouteId": target_bus
    }

    parsed_url = url + "?" + urllib.parse.urlencode(params, encoding="utf-8")
    response = requests.get(parsed_url)
    soup = BeautifulSoup(response.text, "xml")
    items = soup.find_all("item")

    info_lines = []
    for item in items:
        if item.find("stopFlag").text == "0":
            continue
        car_number = item.find("carNo").text
        remain_stops = item.find("stopCount").text
        predict_time = item.find("predictTime").text
        info_lines.append(f"- [{predict_time}ë¶„ í›„] ì°¨ëŸ‰ë²ˆí˜¸ {car_number}, ë‚¨ì€ ì •ë¥˜ì¥ {remain_stops}ê°œ")

    if not info_lines:
        info_lines.append("ë²„ìŠ¤ ë„ì°© ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

    context = "\n".join(info_lines)
    return target_bus, stop_name, context, None

def rag_answer_from_bus(message):
    bus_number, stop_name, arrival_info, error = extract_bus_info_for_rag(message)

    if error:
        return error

    prompt = (
        "[SYSTEM] ë‹¤ìŒ ë²„ìŠ¤ ë„ì°© ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€ë‹µí•´ ì£¼ì„¸ìš”.\n"
        "[TOPIC] ë²„ìŠ¤/í†µí•™\n"
        "[RETRIEVAL]\n"
        f"ë²„ìŠ¤ë²ˆí˜¸: {bus_number}\n"
        f"ì •ë¥˜ì¥ëª…: {stop_name}\n"
        f"ë„ì°© ì •ë³´:\n{arrival_info}\n"
        f"[USER] {message} [SEP] [BOT]"
    )

    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    outputs = model.generate(
        input_ids,
        max_length=input_ids.shape[-1] + 100,
        num_beams=5,
        no_repeat_ngram_size=2,
        early_stopping=True,
        temperature=0.7
    )

    return tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True).strip()
#-------------------------------------------------ë²„ìŠ¤ ìˆ˜ì •í•„ìš”-------------------------------------------------



#-------------------------------------------------ì¡¸ì—… ìš”ê±´----------------------------------------------------
def get_from_graduate(message,topic):
    old_prompt=f"[TOPIC] {topic} [USER] {message} [SEP] [BOT]"
    with open("ì¡¸ì—…ìš”ê±´_RAG.json", encoding="utf-8") as f:
        rag = json.load(f)

    # (2) dept_alias ìë™ ìƒì„± (JSON key ê¸°ë°˜)
    departments = list(rag.get(topic, {}).keys())
    suffixes = ["í•™ê³¼", "í•™ë¶€", "ëŒ€í•™", "êµìœ¡ê³¼"]

    dept_alias = {}
    for name in departments:
        aliases = {name}
        for suf in suffixes:
            if name.endswith(suf) and len(name) > len(suf):
                base = name[:-len(suf)]
                aliases.update({base, base + "í•™", base + "í•™ê³¼"})
        dept_alias[name] = list(aliases)

    # (3) old_promptì—ì„œ dept_key ì°¾ê¸°
    dept_key = next(
        (k for k, als in dept_alias.items() if any(a in old_prompt for a in als)),
        departments[0] if departments else None
    )

    # (4) í•´ë‹¹ í•™ê³¼ JSON ì¶”ì¶œ
    topic_data = rag.get(topic, {})
    dept_data = topic_data.get(dept_key, {})

    # (5) flatten into (path, text)
    chunks = []

    def walk(obj, path=[]):
        if isinstance(obj, dict):
            for k, v in obj.items(): walk(v, path + [k])
        elif isinstance(obj, list):
            for i, item in enumerate(obj): walk(item, path + [str(i)])
        else:
            chunks.append((path, str(obj)))

    walk(dept_data)

    # (6) old_promptì—ì„œ ì–´ì ˆ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = re.findall(r"[ê°€-í£0-9]+", old_prompt)

    # (7) ë¶€ë¶„ë¬¸ìì—´ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ ì²­í¬ë§Œ í•„í„°
    selected = []
    for path, text in chunks:
        if any(kw in key for kw in keywords for key in path) or any(kw in text for kw in keywords):
            selected.append((path, text))

    # (8) Retrieval ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    retrieval = "\n".join(f"{'/'.join(p)}: {t}" for p, t in selected)

    # (9) **SYSTEM** ì§€ì‹œë¬¸ + RAG í”„ë¡¬í”„íŠ¸
    final_prompt = (
        f"[SYSTEM] ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
        f"[TOPIC] {topic}\n"
        f"[RETRIEVAL]\n{retrieval}\n"
        f"[USER] {old_prompt} [SEP]"
    )

    # (10) í† í¬ë‚˜ì´ì €Â·ëª¨ë¸ ë¡œë“œ
    tokenizer = PreTrainedTokenizerFast.from_pretrained("../kogpt2-finetuned")
    model = GPT2LMHeadModel.from_pretrained("../kogpt2-finetuned").eval()

    # (11) ìƒì„±: beam search + ë°˜ë³µ ë°©ì§€ + ë‚®ì€ temperature
    input_ids = tokenizer.encode(final_prompt, return_tensors="pt")
    outs = model.generate(
        input_ids,
        max_length=input_ids.shape[-1] + 100,
        num_beams=5,
        no_repeat_ngram_size=2,
        early_stopping=True,
        do_sample=False,
        temperature=0.5,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

    # (12) ë””ì½”ë”© & ë©”íƒ€í† í° ì œê±°
    raw = tokenizer.decode(outs[0][input_ids.shape[-1]:], skip_special_tokens=True)
    answer = raw.strip()
    return answer
#---------------------------------------------ì¡¸ì—…ìš”ê±´ë-------------------------------------------------



#---------------------------------------------í•™ì‚¬ì¼ì •---------------------------------------------------
# ì¼ë‹¨ í•™ì‚¬ì¼ì • ê°±ì‹ 

def make_rag_context_from_academic_calendar(message):
    with open("academic_calendar.json", encoding="utf-8") as f:
        calendar_data = json.load(f)

    context_lines = []
    for month_info in calendar_data:
        month = month_info["month"]
        for event in month_info["schedules"]:
            text = event["ë‚´ìš©"]
            if any(keyword in message for keyword in [month] + re.findall(r"\d{1,2}ì›”", message)):
                context_lines.append(f"{month}: {text}")
            elif re.search(r"\d{1,2}\.\d{1,2}", message) and re.search(r"\d{1,2}\.\d{1,2}", text):
                context_lines.append(f"{month}: {text}")

    if not context_lines:
        context_lines.append("ê´€ë ¨ëœ í•™ì‚¬ì¼ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return "\n".join(context_lines)

def rag_answer_from_academic_calendar(message):
    context = make_rag_context_from_academic_calendar(message)

    prompt = (
        "[SYSTEM] ì•„ë˜ í•™ì‚¬ì¼ì • ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
        "[TOPIC] í•™ì‚¬ì¼ì •\n"
        f"[RETRIEVAL]\n{context}\n"
        f"[USER] {message} [SEP] [BOT]"
    )

    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    outputs = model.generate(
        input_ids,
        max_length=input_ids.shape[-1] + 100,
        num_beams=5,
        no_repeat_ngram_size=2,
        early_stopping=True,
        temperature=0.5
    )

    return tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True).strip()
#----------------------------------------------í•™ì‚¬ì¼ì •ë--------------------------------------------------
#----------------------------------------------ê³µì§€ì‚¬í•­----------------------------------------------------

def update_notices():
    crawler = CNUNoticeCrawler()
    notices = crawler.crawl_notices(max_pages=10)
    crawler.save_to_json(notices, filename="notices.json")

import json
from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel

# ì „ì—­ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = PreTrainedTokenizerFast.from_pretrained("../kogpt2-finetuned")
model = GPT2LMHeadModel.from_pretrained("../kogpt2-finetuned").eval()

def make_rag_context_from_notices(user_message, top_k=3):
    with open("notices.json", encoding="utf-8") as f:
        notice_data = json.load(f)["data"]

    user_message_lower = user_message.lower()

    # ê°„ë‹¨í•œ keyword matching ê¸°ë°˜ ê²€ìƒ‰ (ì¶”í›„ BM25ë‚˜ FAISSë¡œ êµì²´ ê°€ëŠ¥)
    matches = []
    for item in notice_data:
        score = 0
        title = item["title"].lower()
        content = item["content"].lower()

        if any(word in title for word in user_message_lower.split()):
            score += 1
        if any(word in content for word in user_message_lower.split()):
            score += 2

        if score > 0:
            matches.append((score, item))

    # ìƒìœ„ Kê°œ ì„ íƒ
    matches = sorted(matches, key=lambda x: x[0], reverse=True)[:top_k]

    context_lines = []
    for _, item in matches:
        context_lines.append(f"ğŸ“Œ {item['title']} ({item['date']})")
        context_lines.append(item['content'][:300].replace('\n', ' ') + "...")
        context_lines.append("")

    if not context_lines:
        context_lines.append("ê´€ë ¨ëœ ê³µì§€ì‚¬í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    return "\n".join(context_lines)

def rag_answer_for_notices(user_message):
    retrieval_context = make_rag_context_from_notices(user_message)

    prompt = (
        "[SYSTEM] ì•„ë˜ ê³µì§€ì‚¬í•­ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n"
        "[TOPIC] í•™êµê³µì§€ì‚¬í•­\n"
        "[RETRIEVAL]\n"
        f"{retrieval_context}\n"
        f"[USER] {user_message} [SEP] [BOT]"
    )

    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    output = model.generate(
        input_ids,
        max_length=input_ids.shape[-1] + 100,
        num_beams=5,
        no_repeat_ngram_size=2,
        early_stopping=True,
        temperature=0.8,
    )

    return tokenizer.decode(output[0][input_ids.shape[-1]:], skip_special_tokens=True).strip()
#---------------------------------------------ê³µì§€ì‚¬í•­ ë-------------------------------------------

#---------------------------------------------ë‹µë³€í•˜ê¸°--------------------------------------------
def update_all_data_once():
    global data_updated
    if not data_updated:
        print("ğŸ“Œ ë°ì´í„° ìµœì´ˆ ê°±ì‹  ì¤‘...")
        crawl_academic_calendar()
        update_notices()
        data_updated = True

#ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ topicì„ ìš°ì„  ì¶”ì¶œ
def extract_topic_from_message(message):

    model_dir = "../roberta"

    # ìë™ìœ¼ë¡œ ì €ì¥ëœ í† í¬ë‚˜ì´ì € íƒ€ì…ì„ ë¶ˆëŸ¬ì˜´
    tokenizer_classification = AutoTokenizer.from_pretrained(model_dir)
    model_classification = ElectraForSequenceClassification.from_pretrained(model_dir)
    inputs = tokenizer_classification(message, return_tensors="pt", truncation=True, padding=True)

    # ëª¨ë¸ ì˜ˆì¸¡
    with torch.no_grad():
        outputs = model_classification(**inputs)
        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=1).item()
    id2label = {0: 'ë²„ìŠ¤/í†µí•™', 1: 'ì‹ë‹¨', 2: 'ì¡¸ì—…ìš”ê±´', 3: 'í•™êµê³µì§€ì‚¬í•­', 4: 'í•™ì‚¬ì¼ì •'}
    print(f"ì˜ˆì¸¡ ë¼ë²¨: {id2label[predicted_class]}")
    topic = id2label[predicted_class]

    #ì´í›„ ì˜ˆì¸¡í•œ í† í”½ì„ return
    if topic:
        return topic
    else:
        return None

# respond í•¨ìˆ˜ë§Œ ì´ íŒŒì¼ì— ìµœì¢…ì ìœ¼ë¡œ ë…¸ì¶œ
def respond(message, history=None):
    update_all_data_once()

    if history is None:
        history = []

    topic = extract_topic_from_message(message)

    if topic == "ì‹ë‹¨":
        if "1í•™" in message or "1í•™ìƒíšŒê´€" in message:
            response = rag_answer_from_fixed_menu(message)
        else:
            response = rag_answer_from_menu(message)            # âœ…
    elif topic == "ë²„ìŠ¤/í†µí•™":
        response = rag_answer_from_bus(message)                 # âœ…
    elif topic == "ì¡¸ì—…ìš”ê±´":
        response = get_from_graduate(message, topic)            # âœ…
    elif topic == "í•™ì‚¬ì¼ì •":
        response = rag_answer_from_academic_calendar(message)   # âœ…
    elif topic == "ê³µì§€ì‚¬í•­":
        response = rag_answer_for_notices(message)              # âœ…

    else:
        response = "ì§€ì›ë˜ì§€ ì•ŠëŠ” ì£¼ì œì…ë‹ˆë‹¤."

    history.append({"role": "user", "content": message})
    history.append({"role": "assistant", "content": response})
    return "", history

